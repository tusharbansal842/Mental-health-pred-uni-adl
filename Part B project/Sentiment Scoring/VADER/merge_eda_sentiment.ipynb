{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2e2c9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge_eda4_sentiment_circumplex_risk.ipynb\n",
    "# Purpose: Add sentiment (VADER), valence/arousal (Circumplex via VAD lexicon), and 5-level danger categories.\n",
    "\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import string\n",
    "import warnings\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sentiment (rule-based, robust for social text)\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# If running first time in a new environment:\n",
    "# import nltk; nltk.download('vader_lexicon')\n",
    "\n",
    "# --- Paths ---\n",
    "INPUT_CSV = \"../EDA Code/Merge EDA/all_subreddits_merged_clean.csv\"  # update if needed\n",
    "OUT_DIR   = \"Output\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Optional paths for lexicons (put the files here if you have them)\n",
    "# Option 1: NRC VAD lexicon (preferred): one token per row with V,A,D in [0,1]\n",
    "NRC_VAD_PATH = \"NRC-VAD-Lexicon.txt\"      # expected columns: word, valence, arousal, dominance\n",
    "# Option 2: Warriner norms: has valence/arousal on a 1–9 scale\n",
    "WARRINER_PATH = \"Warriner_et_al_2013.csv\" # expected columns: Word, V.Mean.Sum, A.Mean.Sum\n",
    "\n",
    "# --- Config ---\n",
    "# Candidate text columns to auto-detect\n",
    "TEXT_COLUMNS_CANDIDATES = [\"clean_text\", \"text\", \"body\", \"selftext\", \"post\", \"content\", \"title\"]\n",
    "\n",
    "# Emo/keyword flags for danger detection (non-exhaustive baseline; refine as needed)\n",
    "SELF_HARM_TERMS = [\n",
    "    \"suicide\",\"kill myself\",\"end my life\",\"take my life\",\"self harm\",\"self-harm\",\"cut myself\",\n",
    "    \"overdose\",\"od\",\"hanging\",\"noose\",\"jump off\",\"die by suicide\",\"suicidal\",\"ideation\",\"plan\",\n",
    "    \"kill me\",\"want to die\",\"wish i were dead\",\"i want to die\",\"i dont want to live\",\"i don't want to live\"\n",
    "]\n",
    "CRISIS_TERMS = [\n",
    "    \"urgent\",\"immediately\",\"right now\",\"cant cope\",\"can't cope\",\"panic\",\"panic attack\",\"desperate\",\n",
    "    \"hopeless\",\"worthless\",\"no way out\",\"end it\",\"final note\"\n",
    "]\n",
    "PROTECTIVE_TERMS = [\n",
    "    \"help line\",\"helpline\",\"hotline\",\"therapist\",\"counsellor\",\"counselor\",\"doctor\",\"friend\",\"family\",\"support\"\n",
    "]\n",
    "\n",
    "# VADER thresholds (typical)\n",
    "VADER_NEG = -0.35\n",
    "VADER_POS =  0.35\n",
    "\n",
    "# Circumplex thresholds (valence/arousal scaled to 0–1; adjust later if using Warriner 1–9)\n",
    "VALENCE_LOW  = 0.4\n",
    "VALENCE_HIGH = 0.6\n",
    "AROUSAL_LOW  = 0.4\n",
    "AROUSAL_HIGH = 0.6\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6f274c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using text column: clean_text\n",
      "Rows: 95,250\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "# Try to find a text column\n",
    "text_col = None\n",
    "for c in TEXT_COLUMNS_CANDIDATES:\n",
    "    if c in df.columns:\n",
    "        text_col = c\n",
    "        break\n",
    "\n",
    "if text_col is None:\n",
    "    raise ValueError(f\"Could not find a text column. Looked for: {TEXT_COLUMNS_CANDIDATES}\")\n",
    "\n",
    "print(f\"Using text column: {text_col}\")\n",
    "print(f\"Rows: {len(df):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f88d7462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token column used for VAD: _tokens_for_vad\n"
     ]
    }
   ],
   "source": [
    "# If you already have a 'tokens' column, we’ll use it when computing lexicon matches.\n",
    "# Otherwise we’ll tokenise here in a simple way (lowercase, split on non-letters).\n",
    "\n",
    "TOKEN_COL = \"tokens\" if \"tokens\" in df.columns else None\n",
    "\n",
    "_word_re = re.compile(r\"[a-z']+\")\n",
    "\n",
    "def simple_tokenise(text: str):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    text = text.lower()\n",
    "    return _word_re.findall(text)\n",
    "\n",
    "if TOKEN_COL is None:\n",
    "    df[\"_tokens_for_vad\"] = df[text_col].apply(simple_tokenise)\n",
    "    TOKEN_COL = \"_tokens_for_vad\"\n",
    "else:\n",
    "    # ensure lower-cased tokens\n",
    "    df[\"_tokens_for_vad\"] = df[TOKEN_COL].apply(lambda xs: [x.lower() for x in xs] if isinstance(xs, (list, tuple)) else simple_tokenise(str(xs)))\n",
    "    TOKEN_COL = \"_tokens_for_vad\"\n",
    "\n",
    "print(\"Token column used for VAD:\", TOKEN_COL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f00514c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VADER is ready.\n"
     ]
    }
   ],
   "source": [
    "# Ensures the VADER lexicon is available for NLTK wherever you're running.\n",
    "import os, nltk\n",
    "\n",
    "# Put NLTK data in your home folder so there are no permission issues\n",
    "NLTK_HOME = os.path.join(os.path.expanduser(\"~\"), \"nltk_data\")\n",
    "nltk.data.path.append(NLTK_HOME)\n",
    "\n",
    "try:\n",
    "    # This will raise LookupError if the lexicon is missing\n",
    "    from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "    _ = SentimentIntensityAnalyzer()\n",
    "except LookupError:\n",
    "    print(\"Downloading VADER lexicon to\", NLTK_HOME)\n",
    "    nltk.download(\"vader_lexicon\", download_dir=NLTK_HOME)\n",
    "    from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "    _ = SentimentIntensityAnalyzer()\n",
    "\n",
    "print(\"VADER is ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9d5251b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment_vader</th>\n",
       "      <th>sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.3612</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.9890</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.9565</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.7859</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5719</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment_vader sentiment_label\n",
       "0           0.3612        positive\n",
       "1          -0.9890        negative\n",
       "2           0.9565        positive\n",
       "3           0.7859        positive\n",
       "4           0.5719        positive"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def vader_compound(text):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return 0.0\n",
    "    return sia.polarity_scores(text).get(\"compound\", 0.0)\n",
    "\n",
    "df[\"sentiment_vader\"] = df[text_col].apply(vader_compound)\n",
    "\n",
    "# Optional quick label\n",
    "def vader_label(c):\n",
    "    if c <= VADER_NEG:\n",
    "        return \"negative\"\n",
    "    if c >= VADER_POS:\n",
    "        return \"positive\"\n",
    "    return \"neutral\"\n",
    "\n",
    "df[\"sentiment_label\"] = df[\"sentiment_vader\"].apply(vader_label)\n",
    "df[[\"sentiment_vader\",\"sentiment_label\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "728cfda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK punkt already present.\n",
      "Downloading NLTK punkt_tab to /Users/tusharbansal/nltk_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/tusharbansal/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "import os, nltk\n",
    "NLTK_HOME = os.path.join(os.path.expanduser(\"~\"), \"nltk_data\")\n",
    "nltk.data.path.append(NLTK_HOME)\n",
    "\n",
    "# Ensure both punkt and punkt_tab are present\n",
    "for resource in [\"punkt\", \"punkt_tab\"]:\n",
    "    try:\n",
    "        nltk.data.find(f\"tokenizers/{resource}\")\n",
    "        print(f\"NLTK {resource} already present.\")\n",
    "    except LookupError:\n",
    "        print(f\"Downloading NLTK {resource} to {NLTK_HOME}\")\n",
    "        nltk.download(resource, download_dir=NLTK_HOME)\n",
    "        nltk.data.find(f\"tokenizers/{resource}\")\n",
    "        print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60f93926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>valence_proxy</th>\n",
       "      <th>arousal_proxy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>95250.000</td>\n",
       "      <td>95250.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.440</td>\n",
       "      <td>0.546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.213</td>\n",
       "      <td>0.201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.302</td>\n",
       "      <td>0.445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.425</td>\n",
       "      <td>0.563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.559</td>\n",
       "      <td>0.666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       valence_proxy  arousal_proxy\n",
       "count      95250.000      95250.000\n",
       "mean           0.440          0.546\n",
       "std            0.213          0.201\n",
       "min            0.000          0.000\n",
       "25%            0.302          0.445\n",
       "50%            0.425          0.563\n",
       "75%            0.559          0.666\n",
       "max            1.000          1.000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 4.5 — NRC Emotion → Circumplex proxy (normalised, robustly scaled)\n",
    "import sys, subprocess\n",
    "try:\n",
    "    from nrclex import NRCLex\n",
    "except Exception:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"nrclex\"])\n",
    "    from nrclex import NRCLex\n",
    "\n",
    "# Include positive/negative alongside the 8 basic emotions\n",
    "EMOS_ALL = [\"anger\",\"anticipation\",\"disgust\",\"fear\",\"joy\",\"sadness\",\"surprise\",\"trust\",\"positive\",\"negative\"]\n",
    "\n",
    "def nrc_freqs(text: str):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        d = {e: 0.0 for e in EMOS_ALL}\n",
    "        d[\"emo_total\"] = 0\n",
    "        return d\n",
    "    emo = NRCLex(text.lower())\n",
    "    raw = {e: float(emo.raw_emotion_scores.get(e, 0)) for e in EMOS_ALL}\n",
    "    total = sum(raw.values())\n",
    "    if total == 0:\n",
    "        freq = {e: 0.0 for e in EMOS_ALL}\n",
    "    else:\n",
    "        freq = {e: raw[e] / total for e in EMOS_ALL}\n",
    "    freq[\"emo_total\"] = total\n",
    "    return freq\n",
    "\n",
    "emo_df = df[text_col].apply(nrc_freqs).apply(pd.Series).fillna(0.0)\n",
    "\n",
    "# Heuristic valence/arousal from emotion FREQUENCIES (sum to ~1)\n",
    "val_raw = (\n",
    "    1.00*emo_df[\"positive\"] + 0.80*emo_df[\"joy\"] + 0.60*emo_df[\"trust\"] + 0.35*emo_df[\"anticipation\"] + 0.20*emo_df[\"surprise\"]\n",
    "    - (1.00*emo_df[\"negative\"] + 0.85*emo_df[\"sadness\"] + 0.85*emo_df[\"disgust\"] + 0.75*emo_df[\"fear\"] + 0.60*emo_df[\"anger\"])\n",
    ")\n",
    "\n",
    "aro_raw = (\n",
    "    0.95*emo_df[\"anger\"] + 0.95*emo_df[\"fear\"] + 0.80*emo_df[\"surprise\"] + 0.60*emo_df[\"anticipation\"] + 0.45*emo_df[\"joy\"]\n",
    "    - (0.45*emo_df[\"sadness\"] + 0.35*emo_df[\"disgust\"] + 0.25*emo_df[\"trust\"])\n",
    ")\n",
    "\n",
    "# Robust quantile scaling to 0..1 (avoid a single outlier collapsing spread)\n",
    "def robust_minmax01(s, q_low=0.02, q_high=0.98):\n",
    "    lo, hi = s.quantile(q_low), s.quantile(q_high)\n",
    "    if hi - lo < 1e-9:\n",
    "        return pd.Series(np.zeros(len(s)), index=s.index)\n",
    "    return ((s - lo) / (hi - lo)).clip(0, 1)\n",
    "\n",
    "df[\"valence_proxy\"] = robust_minmax01(val_raw)\n",
    "df[\"arousal_proxy\"] = robust_minmax01(aro_raw)\n",
    "\n",
    "# Quick peek\n",
    "df[[\"valence_proxy\",\"arousal_proxy\"]].describe().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6050e019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAD source: None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hq/6dd_tz_n151_0rsqmh1krf4c0000gn/T/ipykernel_12042/1724309654.py:47: UserWarning: No VAD lexicon found. Place NRC-VAD-Lexicon.txt or Warriner_et_al_2013.csv in the working directory.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vad_lex = {}\n",
    "\n",
    "def load_nrc_vad(path):\n",
    "    # Format assumption: \"word\\tvalence\\tarousal\\tdominance\"\n",
    "    local = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) < 3:\n",
    "                continue\n",
    "            w = parts[0].lower()\n",
    "            try:\n",
    "                v = float(parts[1])\n",
    "                a = float(parts[2])\n",
    "            except ValueError:\n",
    "                continue\n",
    "            # Already in [0,1]\n",
    "            local[w] = (v, a)\n",
    "    return local\n",
    "\n",
    "def load_warriner(path):\n",
    "    # Warriner valence/arousal are on 1–9; we’ll scale to [0,1]\n",
    "    local = {}\n",
    "    tab = pd.read_csv(path)\n",
    "    # Common column names: \"Word\",\"V.Mean.Sum\",\"A.Mean.Sum\"\n",
    "    wcol = \"Word\"\n",
    "    vcol = \"V.Mean.Sum\"\n",
    "    acol = \"A.Mean.Sum\"\n",
    "    for _, row in tab.iterrows():\n",
    "        w = str(row[wcol]).lower()\n",
    "        v = float(row[vcol])\n",
    "        a = float(row[acol])\n",
    "        v01 = (v - 1.0) / 8.0\n",
    "        a01 = (a - 1.0) / 8.0\n",
    "        local[w] = (v01, a01)\n",
    "    return local\n",
    "\n",
    "try:\n",
    "    if os.path.exists(NRC_VAD_PATH):\n",
    "        vad_lex = load_nrc_vad(NRC_VAD_PATH)\n",
    "        source = \"NRC-VAD\"\n",
    "    elif os.path.exists(WARRINER_PATH):\n",
    "        vad_lex = load_warriner(WARRINER_PATH)\n",
    "        source = \"Warriner (scaled)\"\n",
    "    else:\n",
    "        source = None\n",
    "        warnings.warn(\n",
    "            \"No VAD lexicon found. Place NRC-VAD-Lexicon.txt or Warriner_et_al_2013.csv in the working directory.\"\n",
    "        )\n",
    "    print(\"VAD source:\", source, f\"({len(vad_lex):,} entries)\" if vad_lex else \"\")\n",
    "except Exception as e:\n",
    "    vad_lex = {}\n",
    "    source = None\n",
    "    warnings.warn(f\"Failed to load VAD lexicon: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91e5ce7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>valence</th>\n",
       "      <th>arousal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>95250.000</td>\n",
       "      <td>95250.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.440</td>\n",
       "      <td>0.546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.213</td>\n",
       "      <td>0.201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.302</td>\n",
       "      <td>0.445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.425</td>\n",
       "      <td>0.563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.559</td>\n",
       "      <td>0.666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         valence    arousal\n",
       "count  95250.000  95250.000\n",
       "mean       0.440      0.546\n",
       "std        0.213      0.201\n",
       "min        0.000      0.000\n",
       "25%        0.302      0.445\n",
       "50%        0.425      0.563\n",
       "75%        0.559      0.666\n",
       "max        1.000      1.000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 6 — Circumplex: prefer true VAD; otherwise use proxy (already robust-scaled)\n",
    "def post_vad(tokens):\n",
    "    vals, aros = [], []\n",
    "    for t in tokens:\n",
    "        if t in vad_lex:\n",
    "            v, a = vad_lex[t]; vals.append(v); aros.append(a)\n",
    "    if not vals:\n",
    "        return (np.nan, np.nan)\n",
    "    return (float(np.mean(vals)), float(np.mean(aros)))\n",
    "\n",
    "if vad_lex:\n",
    "    vads = df[TOKEN_COL].apply(post_vad)\n",
    "    df[\"valence\"] = vads.apply(lambda x: x[0])\n",
    "    df[\"arousal\"] = vads.apply(lambda x: x[1])\n",
    "else:\n",
    "    df[\"valence\"] = df[\"valence_proxy\"]\n",
    "    df[\"arousal\"] = df[\"arousal_proxy\"]\n",
    "\n",
    "df[[\"valence\",\"arousal\"]].describe().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2430171d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cut-offs: valence LOW=0.354, HIGH=0.502; arousal LOW=0.498, HIGH=0.620\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "circumplex_zone\n",
       "calm/content            14880\n",
       "anxious/panicked        14140\n",
       "distressed              13189\n",
       "energetic               11912\n",
       "relaxed                 11293\n",
       "sad                     10748\n",
       "depressed/lethargic     10134\n",
       "excited/enthusiastic     8954\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 7 — Circumplex zones with data-driven cut-offs (35th/65th percentiles)\n",
    "VALENCE_LOW  = float(df[\"valence\"].quantile(0.35))\n",
    "VALENCE_HIGH = float(df[\"valence\"].quantile(0.65))\n",
    "AROUSAL_LOW  = float(df[\"arousal\"].quantile(0.35))\n",
    "AROUSAL_HIGH = float(df[\"arousal\"].quantile(0.65))\n",
    "\n",
    "def circumplex_zone(v, a):\n",
    "    if np.isnan(v) or np.isnan(a):\n",
    "        return \"unknown\"\n",
    "    if v >= VALENCE_HIGH and a >= AROUSAL_HIGH:\n",
    "        return \"excited/enthusiastic\"\n",
    "    if v >= VALENCE_HIGH and a <= AROUSAL_LOW:\n",
    "        return \"calm/content\"\n",
    "    if v <= VALENCE_LOW and a >= AROUSAL_HIGH:\n",
    "        return \"anxious/panicked\"\n",
    "    if v <= VALENCE_LOW and a <= AROUSAL_LOW:\n",
    "        return \"depressed/lethargic\"\n",
    "    # middles\n",
    "    if v < (VALENCE_LOW + VALENCE_HIGH)/2 and a > (AROUSAL_LOW + AROUSAL_HIGH)/2:\n",
    "        return \"distressed\"\n",
    "    if v < (VALENCE_LOW + VALENCE_HIGH)/2 and a <= (AROUSAL_LOW + AROUSAL_HIGH)/2:\n",
    "        return \"sad\"\n",
    "    if v >= (VALENCE_LOW + VALENCE_HIGH)/2 and a > (AROUSAL_LOW + AROUSAL_HIGH)/2:\n",
    "        return \"energetic\"\n",
    "    return \"relaxed\"\n",
    "\n",
    "df[\"circumplex_zone\"] = df.apply(lambda r: circumplex_zone(r[\"valence\"], r[\"arousal\"]), axis=1)\n",
    "\n",
    "print(\"Cut-offs:\",\n",
    "      f\"valence LOW={VALENCE_LOW:.3f}, HIGH={VALENCE_HIGH:.3f};\",\n",
    "      f\"arousal LOW={AROUSAL_LOW:.3f}, HIGH={AROUSAL_HIGH:.3f}\")\n",
    "df[\"circumplex_zone\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4466f08a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flag_selfharm    0.415\n",
       "flag_crisis      0.113\n",
       "flag_protect     0.255\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_phrase_regex(phrases):\n",
    "    escaped = [re.escape(p.lower()) for p in phrases]\n",
    "    # Word boundary on ends when safe; allow spaces\n",
    "    return re.compile(r\"(\" + \"|\".join(escaped) + r\")\", flags=re.IGNORECASE)\n",
    "\n",
    "re_selfharm = make_phrase_regex(SELF_HARM_TERMS)\n",
    "re_crisis    = make_phrase_regex(CRISIS_TERMS)\n",
    "re_protect   = make_phrase_regex(PROTECTIVE_TERMS)\n",
    "\n",
    "def flag_any(pattern, text):\n",
    "    if not isinstance(text, str) or not text:\n",
    "        return False\n",
    "    return bool(pattern.search(text))\n",
    "\n",
    "df[\"flag_selfharm\"] = df[text_col].apply(lambda t: flag_any(re_selfharm, t))\n",
    "df[\"flag_crisis\"]    = df[text_col].apply(lambda t: flag_any(re_crisis, t))\n",
    "df[\"flag_protect\"]   = df[text_col].apply(lambda t: flag_any(re_protect, t))\n",
    "\n",
    "df[[\"flag_selfharm\",\"flag_crisis\",\"flag_protect\"]].mean().round(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbdfb1c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "danger_label\n",
       "Critical        12156\n",
       "High            36954\n",
       "Low             14011\n",
       "Mild concern    15986\n",
       "Moderate        16143\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Heuristic combiner:\n",
    "# Inputs: VADER, valence/arousal (if available), keyword flags\n",
    "# Output: danger_level: 1..5\n",
    "\n",
    "def danger_level(row):\n",
    "    comp = row[\"sentiment_vader\"]\n",
    "    v    = row.get(\"valence\", np.nan)\n",
    "    a    = row.get(\"arousal\", np.nan)\n",
    "    sh   = bool(row[\"flag_selfharm\"])\n",
    "    cr   = bool(row[\"flag_crisis\"])\n",
    "\n",
    "    # Step 1: Critical indicators (Level 5)\n",
    "    if sh and (\"plan\" in row[text_col].lower() or \"how to\" in row[text_col].lower() or \"goodbye\" in row[text_col].lower()):\n",
    "        return 5\n",
    "    if sh and cr:\n",
    "        return 5\n",
    "\n",
    "    # Step 2: High risk (Level 4)\n",
    "    if sh:\n",
    "        # explicit ideation/self-harm phrases\n",
    "        return 4\n",
    "    if (comp <= -0.7) and (not np.isnan(a) and a >= 0.6):\n",
    "        return 4\n",
    "\n",
    "    # Step 3: Moderate (Level 3)\n",
    "    if (comp <= -0.35 and comp > -0.7):\n",
    "        # Negative sentiment\n",
    "        return 3\n",
    "    if (not np.isnan(v) and v < 0.4) and (not np.isnan(a) and a >= 0.5):\n",
    "        return 3\n",
    "    if cr and comp < 0:\n",
    "        return 3\n",
    "\n",
    "    # Step 4: Mild concern (Level 2)\n",
    "    if comp < 0:\n",
    "        return 2\n",
    "    if (not np.isnan(v) and v < 0.5):\n",
    "        return 2\n",
    "\n",
    "    # Step 5: Low (Level 1)\n",
    "    return 1\n",
    "\n",
    "df[\"danger_level\"] = df.apply(danger_level, axis=1).astype(int)\n",
    "\n",
    "# Human-friendly label (optional)\n",
    "DANGER_LABELS = {\n",
    "    1: \"Low\",\n",
    "    2: \"Mild concern\",\n",
    "    3: \"Moderate\",\n",
    "    4: \"High\",\n",
    "    5: \"Critical\",\n",
    "}\n",
    "df[\"danger_label\"] = df[\"danger_level\"].map(DANGER_LABELS)\n",
    "\n",
    "df[\"danger_label\"].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a12fff3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VADER compound summary:\n",
      "count    95250.000\n",
      "mean        -0.316\n",
      "std          0.699\n",
      "min         -1.000\n",
      "5%          -0.990\n",
      "25%         -0.933\n",
      "50%         -0.648\n",
      "75%          0.318\n",
      "95%          0.940\n",
      "max          1.000\n",
      "Name: sentiment_vader, dtype: float64 \n",
      "\n",
      "Valence summary:\n",
      "count    95250.000\n",
      "mean         0.440\n",
      "std          0.213\n",
      "min          0.000\n",
      "5%           0.098\n",
      "25%          0.302\n",
      "50%          0.425\n",
      "75%          0.559\n",
      "95%          0.845\n",
      "max          1.000\n",
      "Name: valence, dtype: float64 \n",
      "\n",
      "Arousal summary:\n",
      "count    95250.000\n",
      "mean         0.546\n",
      "std          0.201\n",
      "min          0.000\n",
      "5%           0.126\n",
      "25%          0.445\n",
      "50%          0.563\n",
      "75%          0.666\n",
      "95%          0.866\n",
      "max          1.000\n",
      "Name: arousal, dtype: float64 \n",
      "\n",
      "Danger level counts:\n",
      "danger_level\n",
      "1    14011\n",
      "2    15986\n",
      "3    16143\n",
      "4    36954\n",
      "5    12156\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"VADER compound summary:\")\n",
    "print(df[\"sentiment_vader\"].describe(percentiles=[.05,.25,.5,.75,.95]).round(3), \"\\n\")\n",
    "\n",
    "if \"valence\" in df and df[\"valence\"].notna().any():\n",
    "    print(\"Valence summary:\")\n",
    "    print(df[\"valence\"].describe(percentiles=[.05,.25,.5,.75,.95]).round(3), \"\\n\")\n",
    "\n",
    "if \"arousal\" in df and df[\"arousal\"].notna().any():\n",
    "    print(\"Arousal summary:\")\n",
    "    print(df[\"arousal\"].describe(percentiles=[.05,.25,.5,.75,.95]).round(3), \"\\n\")\n",
    "\n",
    "print(\"Danger level counts:\")\n",
    "print(df[\"danger_level\"].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "968deb5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Circumplex availability:\n",
      "  Using VAD source?: NO (proxy)\n",
      "   sentiment_vader   valence   arousal  danger_level\n",
      "0           0.3612  0.492477  0.247432             5\n",
      "1          -0.9890  0.543547  0.340282             2\n",
      "2           0.9565  0.508208  0.530547             5\n",
      "3           0.7859  0.550010  0.482530             5\n",
      "4           0.5719  0.672975  0.687875             1\n",
      "5          -0.3832  0.372398  0.797697             5\n",
      "6          -0.9844  0.384092  0.296919             4\n",
      "7          -0.1280  0.405346  0.573950             2\n"
     ]
    }
   ],
   "source": [
    "print(\"Circumplex availability:\")\n",
    "print(\"  Using VAD source?:\", \"YES\" if len(vad_lex) else \"NO (proxy)\")\n",
    "print(df[[\"sentiment_vader\",\"valence\",\"arousal\",\"danger_level\"]].head(8))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
