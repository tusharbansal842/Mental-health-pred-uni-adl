{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16cb2a49-49b7-4b82-a32e-8f95bb923db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ==== Libraries ====\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "import os\n",
    "\n",
    "# ==== Check device ====\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4164467b-f90e-4e4f-b457-7826eba5f87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shape: (48836, 2)\n",
      "Validation shape: (5427, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My favourite food is anything I didn't have to...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Now if he does off himself, everyone will thin...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WHY THE FUCK IS BAYLESS ISOING</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>To make her feel threatened</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dirty Southern Wankers</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   labels\n",
       "0  My favourite food is anything I didn't have to...  neutral\n",
       "1  Now if he does off himself, everyone will thin...  neutral\n",
       "2                     WHY THE FUCK IS BAYLESS ISOING    anger\n",
       "3                        To make her feel threatened     fear\n",
       "4                             Dirty Southern Wankers    anger"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==== Load 6-class dataset ====\n",
    "train_path = \"../GoemotionsDataset/goemotions_train_grouped.csv\"\n",
    "val_path = \"../GoemotionsDataset/goemotions_val_grouped.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "val_df = pd.read_csv(val_path)\n",
    "\n",
    "print(\"Training shape:\", train_df.shape)\n",
    "print(\"Validation shape:\", val_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b21f10b-96c4-45b7-a1c0-66fed1b0407f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training labels distribution:\n",
      "labels\n",
      "joy         0.378\n",
      "neutral     0.327\n",
      "anger       0.116\n",
      "surprise    0.096\n",
      "sadness     0.057\n",
      "fear        0.014\n",
      "disgust     0.013\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Validation labels distribution:\n",
      "labels\n",
      "joy         0.365\n",
      "neutral     0.329\n",
      "anger       0.122\n",
      "surprise    0.097\n",
      "sadness     0.056\n",
      "fear        0.016\n",
      "disgust     0.015\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# ==== Check label distribution ====\n",
    "print(\"Training labels distribution:\")\n",
    "print(train_df[\"labels\"].value_counts(normalize=True).round(3))\n",
    "\n",
    "print(\"\\nValidation labels distribution:\")\n",
    "print(val_df[\"labels\"].value_counts(normalize=True).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb1b90ee-119a-4fe6-ac91-4debd98f4943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# ==== Tokenizer ====\n",
    "MODEL_NAME = \"mental/mental-bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(\"Tokenizer loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60b1cd2c-80c9-4c2c-950e-0ddeb6be7e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping: {'anger': 0, 'disgust': 1, 'fear': 2, 'joy': 3, 'neutral': 4, 'sadness': 5, 'surprise': 6}\n",
      "Datasets created successfully!\n"
     ]
    }
   ],
   "source": [
    "# ==== Encode string labels into numeric IDs ====\n",
    "label_names = sorted(train_df[\"labels\"].unique())\n",
    "label_to_id = {label: idx for idx, label in enumerate(label_names)}\n",
    "id_to_label = {v: k for k, v in label_to_id.items()}\n",
    "\n",
    "print(\"Label mapping:\", label_to_id)\n",
    "\n",
    "# Apply mapping to both train and validation data\n",
    "train_df[\"labels\"] = train_df[\"labels\"].map(label_to_id)\n",
    "val_df[\"labels\"] = val_df[\"labels\"].map(label_to_id)\n",
    "\n",
    "# ==== PyTorch Dataset class ====\n",
    "class MentalHealthDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=128):\n",
    "        self.texts = df[\"text\"].tolist()\n",
    "        self.labels = df[\"labels\"].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = int(self.labels[idx])  # now safe, all numeric\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        item[\"labels\"] = torch.tensor(label, dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "# ==== Create dataset objects ====\n",
    "train_dataset = MentalHealthDataset(train_df, tokenizer)\n",
    "val_dataset = MentalHealthDataset(val_df, tokenizer)\n",
    "print(\"Datasets created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9604374c-2db0-4a99-b3cf-66e8c23f398d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Metrics ====\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    prec = precision_score(labels, preds, average=\"weighted\")\n",
    "    rec = recall_score(labels, preds, average=\"weighted\")\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": prec, \"recall\": rec}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "986957e2-305c-4fa4-b8c0-2a705313cd31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at mental/mental-bert-base-uncased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialised successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2109/1586246460.py:102: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_final = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# ==== Model training setup for 7-class classification ====\n",
    "from transformers import (\n",
    "    Trainer, TrainingArguments, AutoModelForSequenceClassification, EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "import numpy as np, math, torch, os\n",
    "\n",
    "# ==== Parameters ====\n",
    "best_lr = 5e-5\n",
    "best_wd = 0.01\n",
    "epochs = 10\n",
    "per_device_bs = 16\n",
    "output_dir = \"../MentalBert/mentalbert_fine_tuned_6class_learningrate_5e-5_weightdecay_0.01\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# ==== Metrics ====\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    return {\n",
    "        \"f1\": f1_score(labels, preds, average=\"weighted\"),\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"precision\": precision_score(labels, preds, average=\"weighted\", zero_division=0),\n",
    "        \"recall\": recall_score(labels, preds, average=\"weighted\", zero_division=0),\n",
    "    }\n",
    "\n",
    "# ==== Model ====\n",
    "model_final = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"mental/mental-bert-base-uncased\",\n",
    "    num_labels=7, \n",
    "    problem_type=\"single_label_classification\"\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# ==== TrainingArguments (with version compatibility) ====\n",
    "def build_args():\n",
    "    steps_per_epoch = math.ceil(len(train_dataset) / per_device_bs)\n",
    "    try:\n",
    "        return TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            learning_rate=best_lr,\n",
    "            weight_decay=best_wd,\n",
    "            per_device_train_batch_size=per_device_bs,\n",
    "            per_device_eval_batch_size=per_device_bs,\n",
    "            num_train_epochs=epochs,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"f1\",\n",
    "            greater_is_better=True,\n",
    "            save_total_limit=1,\n",
    "            logging_dir=f\"{output_dir}/logs\",\n",
    "            logging_strategy=\"steps\",\n",
    "            logging_steps=100,\n",
    "            report_to=\"none\",\n",
    "        )\n",
    "    except TypeError:\n",
    "        # fallback for older transformers versions\n",
    "        try:\n",
    "            return TrainingArguments(\n",
    "                output_dir=output_dir,\n",
    "                learning_rate=best_lr,\n",
    "                weight_decay=best_wd,\n",
    "                per_device_train_batch_size=per_device_bs,\n",
    "                per_device_eval_batch_size=per_device_bs,\n",
    "                num_train_epochs=epochs,\n",
    "                eval_strategy=\"epoch\",\n",
    "                save_strategy=\"epoch\",\n",
    "                load_best_model_at_end=True,\n",
    "                metric_for_best_model=\"f1\",\n",
    "                greater_is_better=True,\n",
    "                save_total_limit=1,\n",
    "                logging_dir=f\"{output_dir}/logs\",\n",
    "                logging_steps=100,\n",
    "            )\n",
    "        except TypeError:\n",
    "            # legacy fallback\n",
    "            return TrainingArguments(\n",
    "                output_dir=output_dir,\n",
    "                learning_rate=best_lr,\n",
    "                weight_decay=best_wd,\n",
    "                per_device_train_batch_size=per_device_bs,\n",
    "                per_device_eval_batch_size=per_device_bs,\n",
    "                num_train_epochs=epochs,\n",
    "                do_eval=True,\n",
    "                evaluate_during_training=True,\n",
    "                eval_steps=steps_per_epoch,\n",
    "                save_steps=steps_per_epoch,\n",
    "                save_total_limit=1,\n",
    "                logging_dir=f\"{output_dir}/logs\",\n",
    "                logging_steps=100,\n",
    "            )\n",
    "\n",
    "args_final = build_args()\n",
    "\n",
    "# ==== Early stopping ====\n",
    "early_stopping = EarlyStoppingCallback(early_stopping_patience=2)\n",
    "\n",
    "# ==== Trainer ====\n",
    "trainer_final = Trainer(\n",
    "    model=model_final,\n",
    "    args=args_final,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping],\n",
    ")\n",
    "\n",
    "print(\"Trainer initialised successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ad2e92b-726d-4a8b-a92f-c4c76f3e96be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting fine-tuning for 6-class dataset...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15265' max='30530' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15265/30530 15:51 < 15:51, 16.04 it/s, Epoch 5/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.860200</td>\n",
       "      <td>0.824537</td>\n",
       "      <td>0.673449</td>\n",
       "      <td>0.683803</td>\n",
       "      <td>0.673722</td>\n",
       "      <td>0.683803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.656800</td>\n",
       "      <td>0.907484</td>\n",
       "      <td>0.659541</td>\n",
       "      <td>0.667957</td>\n",
       "      <td>0.672668</td>\n",
       "      <td>0.667957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.444500</td>\n",
       "      <td>1.054325</td>\n",
       "      <td>0.673523</td>\n",
       "      <td>0.675511</td>\n",
       "      <td>0.672609</td>\n",
       "      <td>0.675511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.290500</td>\n",
       "      <td>1.441023</td>\n",
       "      <td>0.648092</td>\n",
       "      <td>0.649714</td>\n",
       "      <td>0.652284</td>\n",
       "      <td>0.649714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.201000</td>\n",
       "      <td>1.986628</td>\n",
       "      <td>0.650487</td>\n",
       "      <td>0.650451</td>\n",
       "      <td>0.655651</td>\n",
       "      <td>0.650451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Model saved successfully to: ../MentalBert/mentalbert_fine_tuned_6class_learningrate_5e-5_weightdecay_0.01/best_model\n",
      "\n",
      " Evaluating best model on validation data...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='340' max='340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [340/340 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final validation metrics:\n",
      "eval_loss: 1.054325\n",
      "eval_f1: 0.673523\n",
      "eval_accuracy: 0.675511\n",
      "eval_precision: 0.672609\n",
      "eval_recall: 0.675511\n",
      "eval_runtime: 7.306800\n",
      "eval_samples_per_second: 742.736000\n",
      "eval_steps_per_second: 46.532000\n",
      "epoch: 5.000000\n"
     ]
    }
   ],
   "source": [
    "# ==== Train the model ====\n",
    "print(\" Starting fine-tuning for 6-class dataset...\\n\")\n",
    "trainer_final.train()\n",
    "\n",
    "# ==== Save the best model ====\n",
    "save_path = f\"{output_dir}/best_model\"\n",
    "trainer_final.save_model(save_path)\n",
    "print(f\"\\n Model saved successfully to: {save_path}\")\n",
    "\n",
    "# ==== Final evaluation on validation set ====\n",
    "print(\"\\n Evaluating best model on validation data...\\n\")\n",
    "final_metrics = trainer_final.evaluate()\n",
    "print(\"Final validation metrics:\")\n",
    "for k, v in final_metrics.items():\n",
    "    print(f\"{k}: {v:.6f}\" if isinstance(v, float) else f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d0eb84e-4083-4d07-a8b5-a5b6a89f8003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels in training set: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6)]\n",
      "Unique labels in validation set: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6)]\n",
      "Expected number of classes: 7\n",
      "\n",
      "After cleanup:\n",
      "Train labels: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6)]\n",
      "Val labels: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6)]\n"
     ]
    }
   ],
   "source": [
    "# ==== Sanity check for label encoding ====\n",
    "print(\"Unique labels in training set:\", sorted(train_df[\"labels\"].unique()))\n",
    "print(\"Unique labels in validation set:\", sorted(val_df[\"labels\"].unique()))\n",
    "print(\"Expected number of classes:\", len(label_to_id))\n",
    "\n",
    "# Remove any rows with missing or out-of-range labels\n",
    "valid_labels = set(range(len(label_to_id)))\n",
    "train_df = train_df[train_df[\"labels\"].isin(valid_labels)]\n",
    "val_df = val_df[val_df[\"labels\"].isin(valid_labels)]\n",
    "\n",
    "print(\"\\nAfter cleanup:\")\n",
    "print(\"Train labels:\", sorted(train_df[\"labels\"].unique()))\n",
    "print(\"Val labels:\", sorted(val_df[\"labels\"].unique()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
